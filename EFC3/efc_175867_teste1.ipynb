{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../EFC3/bloodmnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainX = data[\"train_images\"]\n",
    "TrainY = data[\"train_labels\"]\n",
    "ValX = data[\"val_images\"]\n",
    "ValY = data[\"val_labels\"]\n",
    "TestX = data[\"test_images\"]\n",
    "TestY = data[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dados de treinamento: \n",
      " 28\n",
      "----------------------------------------\n",
      "\n",
      " [[[[0.98039216 0.8745098  0.80784314]\n",
      "   [1.         0.89803922 0.83137255]\n",
      "   [0.98431373 0.87058824 0.80784314]\n",
      "   ...\n",
      "   [1.         0.93333333 0.76470588]\n",
      "   [0.98431373 0.90196078 0.7254902 ]\n",
      "   [1.         0.91764706 0.74901961]]\n",
      "\n",
      "  [[1.         0.89803922 0.82352941]\n",
      "   [1.         0.89803922 0.82352941]\n",
      "   [0.95686275 0.84313725 0.78039216]\n",
      "   ...\n",
      "   [0.99607843 0.90980392 0.75686275]\n",
      "   [0.99607843 0.91372549 0.74509804]\n",
      "   [1.         0.94901961 0.78823529]]\n",
      "\n",
      "  [[1.         0.90588235 0.81568627]\n",
      "   [1.         0.89019608 0.80392157]\n",
      "   [0.93333333 0.82352941 0.74117647]\n",
      "   ...\n",
      "   [1.         0.91372549 0.79215686]\n",
      "   [0.98431373 0.89803922 0.75294118]\n",
      "   [1.         0.94901961 0.79607843]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.91372549 0.8        0.6745098 ]\n",
      "   [0.9372549  0.82352941 0.70588235]\n",
      "   [0.96078431 0.85490196 0.7372549 ]\n",
      "   ...\n",
      "   [1.         0.88235294 0.78039216]\n",
      "   [1.         0.88627451 0.78431373]\n",
      "   [1.         0.89411765 0.78039216]]\n",
      "\n",
      "  [[0.84313725 0.72156863 0.61176471]\n",
      "   [0.85490196 0.7372549  0.62745098]\n",
      "   [0.89411765 0.77647059 0.66666667]\n",
      "   ...\n",
      "   [1.         0.88235294 0.78823529]\n",
      "   [0.99607843 0.89019608 0.78431373]\n",
      "   [1.         0.89411765 0.78823529]]\n",
      "\n",
      "  [[0.85098039 0.71372549 0.63529412]\n",
      "   [0.83137255 0.70588235 0.62352941]\n",
      "   [0.83529412 0.70980392 0.61960784]\n",
      "   ...\n",
      "   [0.99607843 0.88627451 0.8       ]\n",
      "   [1.         0.89019608 0.79607843]\n",
      "   [1.         0.89411765 0.78823529]]]\n",
      "\n",
      "\n",
      " [[[1.         0.90980392 0.83529412]\n",
      "   [0.99607843 0.89019608 0.80784314]\n",
      "   [0.98431373 0.88235294 0.78431373]\n",
      "   ...\n",
      "   [0.68627451 0.5254902  0.55686275]\n",
      "   [0.63137255 0.46666667 0.51372549]\n",
      "   [0.61176471 0.45098039 0.51372549]]\n",
      "\n",
      "  [[0.99215686 0.89019608 0.79215686]\n",
      "   [1.         0.90196078 0.80392157]\n",
      "   [1.         0.90980392 0.80392157]\n",
      "   ...\n",
      "   [0.64705882 0.48627451 0.51764706]\n",
      "   [0.61960784 0.45490196 0.50980392]\n",
      "   [0.61960784 0.45882353 0.52156863]]\n",
      "\n",
      "  [[0.97647059 0.87843137 0.75686275]\n",
      "   [1.         0.90980392 0.78823529]\n",
      "   [1.         0.91764706 0.79607843]\n",
      "   ...\n",
      "   [0.6627451  0.49803922 0.54509804]\n",
      "   [0.65490196 0.49411765 0.55686275]\n",
      "   [0.68235294 0.52156863 0.59215686]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.99607843 0.91372549 0.7372549 ]\n",
      "   [0.99607843 0.91372549 0.74509804]\n",
      "   [0.99607843 0.90980392 0.75686275]\n",
      "   ...\n",
      "   [0.78039216 0.64705882 0.6       ]\n",
      "   [0.78039216 0.64705882 0.6       ]\n",
      "   [0.70980392 0.57647059 0.54117647]]\n",
      "\n",
      "  [[0.99607843 0.91372549 0.74509804]\n",
      "   [0.99607843 0.91372549 0.74509804]\n",
      "   [0.99607843 0.90980392 0.74901961]\n",
      "   ...\n",
      "   [0.77254902 0.62745098 0.59215686]\n",
      "   [0.79215686 0.65882353 0.61960784]\n",
      "   [0.73333333 0.6        0.56470588]]\n",
      "\n",
      "  [[0.99607843 0.90980392 0.76470588]\n",
      "   [0.99607843 0.90980392 0.75686275]\n",
      "   [0.99607843 0.90980392 0.75686275]\n",
      "   ...\n",
      "   [0.71372549 0.56862745 0.54509804]\n",
      "   [0.7372549  0.59215686 0.56862745]\n",
      "   [0.69019608 0.55294118 0.5372549 ]]]\n",
      "\n",
      "\n",
      " [[[0.7254902  0.56862745 0.60392157]\n",
      "   [0.7254902  0.58431373 0.6       ]\n",
      "   [0.79215686 0.65490196 0.64705882]\n",
      "   ...\n",
      "   [0.97647059 0.89019608 0.83529412]\n",
      "   [0.97647059 0.90196078 0.84705882]\n",
      "   [0.97254902 0.89803922 0.83921569]]\n",
      "\n",
      "  [[0.7254902  0.57254902 0.59215686]\n",
      "   [0.76078431 0.61960784 0.61960784]\n",
      "   [0.85490196 0.7254902  0.69803922]\n",
      "   ...\n",
      "   [0.98431373 0.90196078 0.83529412]\n",
      "   [0.97254902 0.88627451 0.83137255]\n",
      "   [0.96470588 0.88235294 0.81568627]]\n",
      "\n",
      "  [[0.80392157 0.65490196 0.64313725]\n",
      "   [0.85490196 0.72156863 0.68627451]\n",
      "   [0.9372549  0.81176471 0.75294118]\n",
      "   ...\n",
      "   [0.96078431 0.87058824 0.8       ]\n",
      "   [0.94509804 0.85490196 0.79215686]\n",
      "   [0.94117647 0.85098039 0.78039216]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         0.89803922 0.80392157]\n",
      "   [0.99215686 0.88235294 0.79607843]\n",
      "   [1.         0.89019608 0.80784314]\n",
      "   ...\n",
      "   [0.87058824 0.72156863 0.71764706]\n",
      "   [0.83529412 0.68627451 0.68235294]\n",
      "   [0.83529412 0.68627451 0.68235294]]\n",
      "\n",
      "  [[1.         0.89411765 0.8       ]\n",
      "   [1.         0.89019608 0.79607843]\n",
      "   [1.         0.90196078 0.81960784]\n",
      "   ...\n",
      "   [0.87843137 0.72156863 0.7254902 ]\n",
      "   [0.84313725 0.68627451 0.69019608]\n",
      "   [0.84313725 0.68627451 0.69019608]]\n",
      "\n",
      "  [[1.         0.89019608 0.79607843]\n",
      "   [1.         0.89019608 0.79607843]\n",
      "   [1.         0.89803922 0.81568627]\n",
      "   ...\n",
      "   [0.8627451  0.70588235 0.70588235]\n",
      "   [0.83529412 0.67843137 0.67843137]\n",
      "   [0.83921569 0.68235294 0.68627451]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.94901961 0.9372549  0.6745098 ]\n",
      "   [0.93333333 0.91372549 0.68627451]\n",
      "   [0.97647059 0.92941176 0.76470588]\n",
      "   ...\n",
      "   [0.98039216 0.8627451  0.8627451 ]\n",
      "   [0.96862745 0.84313725 0.85490196]\n",
      "   [0.97254902 0.84705882 0.85098039]]\n",
      "\n",
      "  [[0.97647059 0.94117647 0.71764706]\n",
      "   [0.96862745 0.92941176 0.72941176]\n",
      "   [0.98431373 0.92941176 0.77647059]\n",
      "   ...\n",
      "   [0.88235294 0.76862745 0.75294118]\n",
      "   [0.86666667 0.74509804 0.7372549 ]\n",
      "   [0.86666667 0.74509804 0.7372549 ]]\n",
      "\n",
      "  [[0.97647059 0.90588235 0.74901961]\n",
      "   [0.98431373 0.90588235 0.76078431]\n",
      "   [1.         0.9254902  0.79607843]\n",
      "   ...\n",
      "   [0.81568627 0.70588235 0.6627451 ]\n",
      "   [0.8        0.68235294 0.65098039]\n",
      "   [0.8        0.68235294 0.64313725]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         0.91372549 0.8745098 ]\n",
      "   [0.99607843 0.90196078 0.8627451 ]\n",
      "   [0.96862745 0.8627451  0.81960784]\n",
      "   ...\n",
      "   [0.75294118 0.63921569 0.58431373]\n",
      "   [0.81568627 0.70196078 0.64705882]\n",
      "   [0.90980392 0.78431373 0.74117647]]\n",
      "\n",
      "  [[1.         0.89019608 0.8745098 ]\n",
      "   [0.96078431 0.85098039 0.83529412]\n",
      "   [0.88627451 0.77254902 0.75686275]\n",
      "   ...\n",
      "   [0.78431373 0.65490196 0.61960784]\n",
      "   [0.79215686 0.6627451  0.63529412]\n",
      "   [0.85098039 0.72156863 0.69411765]]\n",
      "\n",
      "  [[0.98431373 0.87058824 0.84705882]\n",
      "   [0.93333333 0.81960784 0.79607843]\n",
      "   [0.84313725 0.72156863 0.70196078]\n",
      "   ...\n",
      "   [0.81176471 0.68235294 0.6627451 ]\n",
      "   [0.78823529 0.65098039 0.63529412]\n",
      "   [0.83137255 0.69411765 0.68627451]]]\n",
      "\n",
      "\n",
      " [[[0.80392157 0.6627451  0.67843137]\n",
      "   [0.85098039 0.70980392 0.70980392]\n",
      "   [0.9254902  0.78823529 0.76470588]\n",
      "   ...\n",
      "   [1.         0.85882353 0.8745098 ]\n",
      "   [0.95686275 0.80784314 0.85098039]\n",
      "   [0.95294118 0.80392157 0.84705882]]\n",
      "\n",
      "  [[0.75686275 0.61176471 0.63921569]\n",
      "   [0.79607843 0.65490196 0.6627451 ]\n",
      "   [0.8745098  0.7372549  0.72156863]\n",
      "   ...\n",
      "   [0.88627451 0.74509804 0.75294118]\n",
      "   [0.84705882 0.70196078 0.72941176]\n",
      "   [0.83921569 0.69411765 0.7254902 ]]\n",
      "\n",
      "  [[0.75686275 0.60784314 0.65098039]\n",
      "   [0.76078431 0.61568627 0.64313725]\n",
      "   [0.81568627 0.6745098  0.6745098 ]\n",
      "   ...\n",
      "   [0.8        0.6627451  0.65490196]\n",
      "   [0.77254902 0.63137255 0.63921569]\n",
      "   [0.76862745 0.62745098 0.63529412]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.82745098 0.68235294 0.64705882]\n",
      "   [0.80784314 0.6627451  0.63137255]\n",
      "   [0.78823529 0.63921569 0.62745098]\n",
      "   ...\n",
      "   [0.99215686 0.87843137 0.80784314]\n",
      "   [0.99215686 0.87843137 0.81568627]\n",
      "   [0.99215686 0.87843137 0.81568627]]\n",
      "\n",
      "  [[0.81960784 0.6745098  0.65098039]\n",
      "   [0.8        0.65098039 0.63921569]\n",
      "   [0.78039216 0.62745098 0.63137255]\n",
      "   ...\n",
      "   [0.99607843 0.88235294 0.81960784]\n",
      "   [1.         0.88627451 0.83137255]\n",
      "   [1.         0.88627451 0.83137255]]\n",
      "\n",
      "  [[0.8        0.64313725 0.64313725]\n",
      "   [0.78039216 0.62352941 0.62745098]\n",
      "   [0.76862745 0.61568627 0.62745098]\n",
      "   ...\n",
      "   [0.98431373 0.87058824 0.81568627]\n",
      "   [0.98823529 0.87058824 0.82745098]\n",
      "   [0.99215686 0.8745098  0.83137255]]]\n",
      "\n",
      "\n",
      " [[[0.76862745 0.62352941 0.6       ]\n",
      "   [0.75294118 0.60392157 0.6       ]\n",
      "   [0.75686275 0.60392157 0.62352941]\n",
      "   ...\n",
      "   [0.84705882 0.70980392 0.70196078]\n",
      "   [0.89019608 0.74901961 0.75686275]\n",
      "   [0.90980392 0.76862745 0.77647059]]\n",
      "\n",
      "  [[0.77254902 0.63137255 0.57647059]\n",
      "   [0.75294118 0.60784314 0.57254902]\n",
      "   [0.75294118 0.6        0.60392157]\n",
      "   ...\n",
      "   [0.84313725 0.70588235 0.69019608]\n",
      "   [0.8745098  0.73333333 0.73333333]\n",
      "   [0.88627451 0.74509804 0.75294118]]\n",
      "\n",
      "  [[0.78039216 0.64705882 0.54117647]\n",
      "   [0.76078431 0.62352941 0.54901961]\n",
      "   [0.75294118 0.60784314 0.57647059]\n",
      "   ...\n",
      "   [0.82745098 0.69019608 0.66666667]\n",
      "   [0.83921569 0.70196078 0.69411765]\n",
      "   [0.84705882 0.70588235 0.70588235]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.         0.89019608 0.82745098]\n",
      "   [1.         0.89019608 0.81960784]\n",
      "   [1.         0.88627451 0.81568627]\n",
      "   ...\n",
      "   [0.79215686 0.63921569 0.61176471]\n",
      "   [0.81568627 0.65882353 0.65098039]\n",
      "   [0.86666667 0.70980392 0.70980392]]\n",
      "\n",
      "  [[1.         0.89019608 0.82745098]\n",
      "   [1.         0.88627451 0.82352941]\n",
      "   [1.         0.88627451 0.82352941]\n",
      "   ...\n",
      "   [0.76470588 0.61960784 0.58823529]\n",
      "   [0.81176471 0.65490196 0.64705882]\n",
      "   [0.88627451 0.72941176 0.72941176]]\n",
      "\n",
      "  [[1.         0.88627451 0.82352941]\n",
      "   [1.         0.88627451 0.82352941]\n",
      "   [1.         0.88627451 0.82352941]\n",
      "   ...\n",
      "   [0.76470588 0.61960784 0.59607843]\n",
      "   [0.80784314 0.65882353 0.65490196]\n",
      "   [0.87843137 0.72941176 0.7254902 ]]]]\n"
     ]
    }
   ],
   "source": [
    "print(f'dados de treinamento: \\n {len(TrainX[0])}')\n",
    "\n",
    "print('--'*20)\n",
    "print(f'\\n {TrainX/255}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainX_n = TrainX/255\n",
    "TestX_n = TestX/255\n",
    "ValX_n = ValX/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.4288 - loss: 1.5143 - val_accuracy: 0.5456 - val_loss: 1.0985\n",
      "Epoch 2/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6059 - loss: 1.0491 - val_accuracy: 0.6869 - val_loss: 0.8519\n",
      "Epoch 3/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6560 - loss: 0.9215 - val_accuracy: 0.6197 - val_loss: 0.9576\n",
      "Epoch 4/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6956 - loss: 0.8340 - val_accuracy: 0.7249 - val_loss: 0.7437\n",
      "Epoch 5/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7080 - loss: 0.8097 - val_accuracy: 0.7529 - val_loss: 0.6787\n",
      "Epoch 6/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7315 - loss: 0.7321 - val_accuracy: 0.6928 - val_loss: 0.7667\n",
      "Epoch 7/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7330 - loss: 0.7251 - val_accuracy: 0.7588 - val_loss: 0.6523\n",
      "Epoch 8/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7536 - loss: 0.6879 - val_accuracy: 0.6641 - val_loss: 0.8821\n",
      "Epoch 9/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7416 - loss: 0.7006 - val_accuracy: 0.7296 - val_loss: 0.7452\n",
      "Epoch 10/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7655 - loss: 0.6490 - val_accuracy: 0.7798 - val_loss: 0.5907\n",
      "Epoch 11/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7604 - loss: 0.6570 - val_accuracy: 0.7553 - val_loss: 0.6630\n",
      "Epoch 12/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7733 - loss: 0.6329 - val_accuracy: 0.8119 - val_loss: 0.5499\n",
      "Epoch 13/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7788 - loss: 0.6002 - val_accuracy: 0.8037 - val_loss: 0.5344\n",
      "Epoch 14/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7841 - loss: 0.5921 - val_accuracy: 0.7167 - val_loss: 0.7463\n",
      "Epoch 15/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7924 - loss: 0.5705 - val_accuracy: 0.8061 - val_loss: 0.5387\n",
      "Epoch 16/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7878 - loss: 0.5774 - val_accuracy: 0.7494 - val_loss: 0.6444\n",
      "Epoch 17/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7932 - loss: 0.5756 - val_accuracy: 0.8213 - val_loss: 0.4993\n",
      "Epoch 18/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7987 - loss: 0.5613 - val_accuracy: 0.7845 - val_loss: 0.5670\n",
      "Epoch 19/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8030 - loss: 0.5607 - val_accuracy: 0.8248 - val_loss: 0.4814\n",
      "Epoch 20/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7931 - loss: 0.5565 - val_accuracy: 0.7979 - val_loss: 0.5590\n",
      "Epoch 21/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8081 - loss: 0.5227 - val_accuracy: 0.8178 - val_loss: 0.4942\n",
      "Epoch 22/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8131 - loss: 0.5210 - val_accuracy: 0.7932 - val_loss: 0.5699\n",
      "Epoch 23/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8094 - loss: 0.5127 - val_accuracy: 0.8265 - val_loss: 0.4890\n",
      "Epoch 24/30\n",
      "\u001b[1m1495/1495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8137 - loss: 0.5241 - val_accuracy: 0.8119 - val_loss: 0.5234\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8364 - loss: 0.4547\n",
      "Batch Size: 8, Acurácia da validação: 0.8247663378715515\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(ValX_n,ValY)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Acurácia da validação: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mval_acc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbest_accuracy\u001b[49m:\n\u001b[0;32m     25\u001b[0m    best_accuracy \u001b[38;5;241m=\u001b[39m val_acc\n\u001b[0;32m     26\u001b[0m    best_size \u001b[38;5;241m=\u001b[39m batch_size\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "batchs_size = [8, 16, 32, 64, 128]\n",
    "best_accuracy = 0\n",
    "best_size = None\n",
    "\n",
    "for batch_size in batchs_size:\n",
    "    \n",
    "   model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(28, 28, 3)),\n",
    "    tf.keras.layers.Flatten(),   # Camada de entrada\n",
    "    tf.keras.layers.Dense(128, activation='relu'),      # Camada intermediária\n",
    "    tf.keras.layers.Dense(8,activation='softmax')       # Camada de saída\n",
    "    ])\n",
    "   \n",
    "   model.compile(optimizer='SGD',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "   \n",
    "   hist = model.fit(TrainX_n, TrainY, batch_size=batch_size, epochs=30, verbose=1, validation_data=(ValX_n, ValY), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True))\n",
    "\n",
    "   val_acc = model.evaluate(ValX_n,ValY)\n",
    "\n",
    "   print(f'Batch Size: {batch_size}, Acurácia da validação: {val_acc[1]}')\n",
    "\n",
    "   if val_acc[1] > best_accuracy:\n",
    "      best_accuracy = val_acc[1]\n",
    "      best_size = batch_size\n",
    "\n",
    "\n",
    "print(f'O melhor valor de Batch é: {best_size}, com uma acurácia de validação de {best_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(28, 28, 3)),\n",
    "    tf.keras.layers.Flatten(),   # Camada de entrada\n",
    "    tf.keras.layers.Dense(128, activation='relu'),      # Camada intermediária\n",
    "    tf.keras.layers.Dense(8,activation='softmax')       # Camada de saída\n",
    "    ])\n",
    "   \n",
    "model.compile(optimizer='SGD',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "   \n",
    "hist = model.fit(TrainX_n, TrainY, batch_size=best_size, epochs=30, verbose=1, validation_data=(ValX_n, ValY), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "validation_acc = hist.history['val_accuracy']\n",
    "\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "range_epochs = np.arange(15)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range_epochs,acc, label='Acuracia Treinamento')\n",
    "plt.plot(range_epochs,validation_acc, label='Acuracia Validação')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range_epochs,loss, label='Função Custo Treinamento')\n",
    "plt.plot(range_epochs,val_loss, label='Função Custo Validação')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "#Plot the confusion matrix. Set Normalize = True/False\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.around(cm, decimals=2)\n",
    "        cm[np.isnan(cm)] = 0.0\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "pred = model.predict(TestX_n)\n",
    "\n",
    "y_pred = np.empty((len(pred),1),dtype=int)\n",
    "for i in range(len(pred)):\n",
    "    y_pred[i,0] = np.argmax(pred[i])\n",
    "\n",
    "score = model.evaluate(TestX,TestY)\n",
    "\n",
    "print(f'Função-Custo: {score[0]}')\n",
    "print(f'Acurácia: {score[1]}')\n",
    "\n",
    "print('--'*50)\n",
    "\n",
    "cm = confusion_matrix(TestY,y_pred)\n",
    "plot_confusion_matrix(cm, ['0', '1', '2', '3', '4', '5', '6', '7'], normalize=False, title='Confusion Matrix')\n",
    "\n",
    "# confusionMatrix = pd.DataFrame(data=cm, index=['0,true', '1,true', '2,true', '3,true', '4,true', '5,true', '6,true', '7,true'], columns=['0, pred', '1, pred', '2, pred', '3, pred', '4, pred', '5, pred', '6, pred', '7, pred'])\n",
    "# confusionMatrix.loc['sum'] = confusionMatrix.sum()\n",
    "# confusionMatrix['sum'] = confusionMatrix.sum(axis=1)\n",
    "# print(confusionMatrix)\n",
    "# print('--'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
